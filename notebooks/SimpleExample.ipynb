{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SimpleExample.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ulKHpLRCBJgl"},"source":["# This code contains a simple example using the function `PCInfer` from `Torch2PC` for training a convolutional neural network on MNIST.\n","\n","The first code cell imports the MNIST data and defines some hyperparameters, but contains nothing specific to `Torch2PC`.\n","\n","If you use this code, please cite this paper:\n","[https://arxiv.org/abs/2106.13082](https://arxiv.org/abs/2106.13082)\n","\n"]},{"cell_type":"code","metadata":{"id":"VcEy5twAUPkq"},"source":["import torch \n","import torch.nn as nn\n","import numpy as np\n","import torchvision \n","import matplotlib.pyplot as plt\n","from time import time as tm\n","\n","# Import TorchSeq2PC \n","!git clone https://github.com/RobertRosenbaum/Torch2PC.git\n","from Torch2PC import TorchSeq2PC as T2PC  \n","\n","# Seed rng\n","torch.manual_seed(0)\n","\n","# # This patches an error that sometimes arises in\n","# # downloading MNIST\n","# from six.moves import urllib\n","# opener = urllib.request.build_opener()\n","# opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n","# urllib.request.install_opener(opener)\n","\n","# This seems to be a more reliable and faster\n","# source for MNIST\n","!wget -nc www.di.ens.fr/~lelarge/MNIST.tar.gz\n","!tar -zxvf MNIST.tar.gz\n","\n","# Load training and testing data from MNIST dataset\n","# These lines return data structures that contain\n","# the training and testing data \n","from torchvision.datasets import MNIST\n","\n","# Get training data structure\n","train_dataset = MNIST('./', \n","      train=True, \n","      transform=torchvision.transforms.ToTensor(),  \n","      download=True)\n","\n","# Number of trainin data points\n","m = len(train_dataset)\n","\n","# Print the size of the training data set\n","print('\\n\\n\\n')\n","print(\"Number of data points in training set = \",m)\n","print(\"Size of training inputs (X)=\",train_dataset.data.size())\n","print(\"Size of training labels (Y)=\",train_dataset.targets.size())\n","\n","# Define batch size\n","batch_size = 300      # Batch size to use with training data\n","\n","# Create data loader. \n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n","                                          batch_size=batch_size, \n","                                          shuffle=True)\n","\n","\n","# Choose device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('device = ',device)\n","\n","# Define the nunmber of epochs, learning rate, \n","# and how often to print progress\n","num_epochs=2\n","LearningRate=.002\n","PrintEvery=50\n","\n","# Choose an optimizer\n","WhichOptimizer=torch.optim.Adam\n","\n","# Compute size of each batch\n","steps_per_epoch = len(train_loader) \n","total_num_steps  = num_epochs*steps_per_epoch\n","print(\"steps per epoch (mini batch size)=\",steps_per_epoch)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0fxLn1PGBlTY"},"source":["# The next code cell builds a convolutional neural network model using `Sequential`. \n","\n","The `PCInfer` function treats each element of a `Sequential` model as a layer. As such, it is necessary to use nested calls to `Sequential` (as below) if you want to treat a block of functions as a layer. For the model below, each block will be treated as a layer (5 layers in all)."]},{"cell_type":"code","metadata":{"id":"xrqeO3VeURXa"},"source":["\n","# Define model using Sequential. \n","model=nn.Sequential(\n","    \n","    nn.Sequential(nn.Conv2d(1,10,3),\n","    nn.ReLU(),\n","    nn.MaxPool2d(2)\n","    ),\n","\n","    nn.Sequential(\n","    nn.Conv2d(10,5,3),\n","    nn.ReLU(),\n","    nn.Flatten()\n","    ),\n","\n"," nn.Sequential(    \n","    nn.Linear(5*11*11,50),\n","    nn.ReLU()\n","    ),\n","\n"," nn.Sequential(    \n","    nn.Linear(50,30),\n","    nn.ReLU()\n","    ),\n","\n","\n","nn.Sequential(\n","   nn.Linear(30,10)\n"," )\n","\n",").to(device)\n","\n","# Define the loss function\n","LossFun = nn.CrossEntropyLoss()\n","\n","# Compute one batch of output and loss to make sure\n","# things are working\n","with torch.no_grad():\n","  TrainingIterator=iter(train_loader)\n","  X,Y=next(TrainingIterator)  \n","  X=X.to(device)\n","  Y=Y.to(device)\n","  Yhat=model(X).to(device)\n","  print('output shape = ',Yhat.shape)\n","  print('loss on initial model = ',LossFun(Yhat,Y).item())\n","\n","\n","NumParams=sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print('Number of trainable parameters in model =',NumParams)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nfl9P_A6CLa8"},"source":["# The next code cell defines hyperparameters for `PCInfer`\n","\n","The hyperparameter `ErrType` controls which algorithm to use for computing the beliefs and prediction errors. It should be equal to `'Strict'`, `'FixedPred'`, or `'Exact'`. `'Strict'` uses a strict interpretation of predictive coding (without the fixed prediction assumption), `'FixedPred'` uses the fixed prediction assumption, and `'Exact'` computes the exact gradients (same as those computed by backpropagation). See \"On the relationship between predictive coding and backpropagation\" for more information on these algorithms.\n","\n","`eta` and `n` are the step size and number of steps to use for the iterations that compute the prediction errors and beliefs. These parameters are not used when `ErrType='Exact'`"]},{"cell_type":"code","metadata":{"id":"gYYDZD6iN-dO"},"source":["# Define PC hyperparameters\n","\n","ErrType=\"Strict\"\n","eta=.1\n","n=20"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jm0Z6rK-Cmua"},"source":["# The next code cell uses `PCInfer` to train the model\n","\n","The only line that differs from a typical training loop in PyTorch is the line\n","\n","`\n","vhat,Loss,dLdy,v,epsilon=T2PC.PCInfer(model,LossFun,X,Y,ErrType,eta,n)\n","`\n","\n","which computes the outputs, loss, etc. and it sets the `.grad` attributes of all parameters in `model` to the parameter update values computed by predictive coding. \n","\n","For `ErrType='Exact'`, the gradients are set to the gradient of the loss with respect to that parameter, i.e., the same values computed by calling `Loss.backward()` after a single forward pass. For other values of `ErrType`, refer to the paper for an explanation of how the parameter updates are computed."]},{"cell_type":"code","metadata":{"id":"NBqT3hYHUXAC"},"source":["\n","# Define the optimizer\n","optimizer = WhichOptimizer(model.parameters(), lr=LearningRate)\n","\n","# Initialize vector to store losses\n","LossesToPlot=np.zeros(total_num_steps)\n","\n","\n","j=0     # Counters\n","jj=0    \n","t1=tm() # Get start time\n","for k in range(num_epochs):\n","\n","  # Re-initialize the training iterator (shuffles data for one epoch)\n","  TrainingIterator=iter(train_loader)\n","  \n","  for i in range(steps_per_epoch): # For each batch\n","\n","    # Get one batch of training data, reshape it\n","    # and send it to the current device        \n","    X,Y=next(TrainingIterator)  \n","    X=X.to(device)\n","    Y=Y.to(device)\n","\n","    # Perform inference on this batch\n","    vhat,Loss,dLdy,v,epsilon=T2PC.PCInfer(model,LossFun,X,Y,ErrType,eta,n)\n","\n","    # Update parameters    \n","    optimizer.step() \n","\n","    # Zero-out gradients     \n","    model.zero_grad()\n","    optimizer.zero_grad()\n","\n","    # Print and store loss\n","    with torch.no_grad():\n","      if(i%PrintEvery==0):\n","        print('epoch =',k,'step =',i,'Loss =',Loss.item())\n","      LossesToPlot[jj]=Loss.item() \n","      jj+=1\n","\n","# Compute and print time spent training\n","tTrain=tm()-t1\n","print('Training time = ',tTrain,'sec')\n","\n","# Plot the loss curve\n","plt.figure()\n","plt.plot(LossesToPlot)\n","plt.ylim(bottom=0)  \n","plt.ylabel('training loss')\n","plt.xlabel('iteration number')\n","\n"],"execution_count":null,"outputs":[]}]}